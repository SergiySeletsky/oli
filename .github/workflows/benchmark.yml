name: Benchmark
on:
  workflow_dispatch:
    inputs:
      model:
        description: 'Model to benchmark'
        default: 'qwen2.5-coder:7b'
        required: true
      prompt:
        description: 'Test prompt to send'
        default: 'Hello, how are you today?'
        required: true
  push:
    branches: [main]
    paths:
      - '.github/workflows/benchmark.yml'
  pull_request:
    branches: [main]
    paths:
      - '.github/workflows/benchmark.yml'

permissions:
  contents: read
  id-token: write
  actions: read

jobs:
  benchmark:
    name: Benchmark Ollama with oli
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          toolchain: stable

      - name: Set defaults for non-workflow_dispatch events
        if: github.event_name != 'workflow_dispatch'
        run: |
          echo "MODEL=qwen2.5-coder:7b" >> $GITHUB_ENV
          echo "PROMPT=Hello, how are you today?" >> $GITHUB_ENV

      - name: Set values from workflow inputs
        if: github.event_name == 'workflow_dispatch'
        run: |
          echo "MODEL=${{ github.event.inputs.model }}" >> $GITHUB_ENV
          echo "PROMPT=${{ github.event.inputs.prompt }}" >> $GITHUB_ENV

      # Cache the Ollama models to avoid redownloading each time
      - name: Cache Ollama models
        id: cache-ollama
        uses: actions/cache@v4
        with:
          path: ~/.ollama/models
          key: ollama-${{ env.MODEL }}-${{ hashFiles('.github/workflows/benchmark.yml') }}

      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          ollama --version

      - name: Start Ollama server
        id: server
        run: |
          # Create Ollama directories if the cache missed
          mkdir -p ~/.ollama/models

          # Start Ollama in background
          ollama serve &
          echo "OLLAMA_PID=$!" >> $GITHUB_ENV

          # Wait for server to be ready
          echo "Waiting for Ollama server to start..."
          start_time=$(date +%s)
          max_wait=30

          until curl -s http://localhost:11434/api/tags > /dev/null 2>&1; do
            sleep 1
            elapsed=$(($(date +%s) - start_time))
            if [ $elapsed -ge $max_wait ]; then
              echo "::error::Timed out waiting for Ollama server to start"
              exit 1
            fi
          done

          echo "Ollama server started successfully"

      - name: Pull model
        run: |
          echo "Pulling model ${{ env.MODEL }}..."
          if [ "${{ steps.cache-ollama.outputs.cache-hit }}" == "true" ]; then
            echo "Using cached model"
          fi
          ollama pull ${{ env.MODEL }}

      - name: Build oli server
        run: |
          echo "Building oli server..."
          # Run faster build for CI
          cargo build --release --bin oli-server
          echo "oli server built successfully"

      - name: Create results directory
        run: |
          # Create results directory
          mkdir -p benchmark_results
          mkdir -p benchmark_results/tool_tests

      # Instead of modifying code, use a temporary API key
      - name: Set API keys for testing
        run: |
          # Create a dummy API key for testing
          echo "ANTHROPIC_API_KEY=dummy-key-for-testing-only" >> $GITHUB_ENV
          echo "OPENAI_API_KEY=dummy-key-for-testing-only" >> $GITHUB_ENV
          echo "GEMINI_API_KEY=dummy-key-for-testing-only" >> $GITHUB_ENV

          # Create config directory
          mkdir -p ~/.config/oli

          # Simplified config file with Ollama as default provider
          cat > ~/.config/oli/config.json << EOF
          {
            "default_provider": "ollama",
            "default_model": "${{ env.MODEL }}"
          }
          EOF

          echo "Created test config and dummy API keys"

      # No oli server test - focusing only on tool benchmark

      - name: Run tool benchmark tests
        id: tool_benchmark
        run: |
          echo "Running tool benchmark tests with model ${{ env.MODEL }}"

          # Set environment variables for the tests
          export OLLAMA_API_BASE="http://localhost:11434"
          export DEFAULT_PROVIDER="ollama"
          export DEFAULT_MODEL="${{ env.MODEL }}"

          # Create results directory for benchmark tests
          mkdir -p benchmark_results/tool_tests

          # Run the benchmark tests (with feature flag to enable them)
          START=$(date +%s%3N)
          RESULT=$(cargo test --test integration test_file_read_tool --features benchmark -- --nocapture 2>&1)
          END=$(date +%s%3N)
          TIME=$((END - START))

          # Save test results
          echo "$RESULT" > benchmark_results/tool_tests/file_read_tool_results.txt

          # Create a summary JSON
          cat > benchmark_results/tool_tests/summary.json << EOF
          {
            "metadata": {
              "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
              "model": "${{ env.MODEL }}",
              "test_type": "file_read_tool_benchmark"
            },
            "metrics": {
              "execution_time_ms": $TIME
            },
            "result": "completed"
          }
          EOF

          # Output test completion message
          echo "Tool benchmark tests completed in ${TIME}ms"
          echo "tool_benchmark_time=${TIME}" >> $GITHUB_OUTPUT

      - name: Generate summary
        run: |
          # Get tool benchmark time
          TOOL_BENCHMARK_TIME="${{ steps.tool_benchmark.outputs.tool_benchmark_time }}"

          # Create summary file
          cat > benchmark_results/summary.json << EOF
          {
            "tool_benchmark_ms": $TOOL_BENCHMARK_TIME,
            "model": "${{ env.MODEL }}",
            "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
          }
          EOF

          # Output summary to console
          echo "=== Benchmark Results ==="
          echo "Tool benchmark tests: ${TOOL_BENCHMARK_TIME}ms"
          echo "Model: ${{ env.MODEL }}"

          # Print tool benchmark summary
          echo "=== Tool Benchmark Results ==="
          if [ -f benchmark_results/tool_tests/file_read_tool_results.txt ]; then
            grep -E "test |running|result:|BENCHMARK SUCCESS" benchmark_results/tool_tests/file_read_tool_results.txt
          else
            echo "No tool benchmark results found"
          fi

      - name: Upload results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results
          path: |
            benchmark_results/
            ~/.config/oli/config.json
            benchmark_results/tool_tests/

      - name: Update benchmark documentation
        if: github.event_name == 'pull_request'
        run: |
          # Make script executable
          chmod +x .github/scripts/update_benchmark_docs.sh

          # Run the script to update benchmark.md
          .github/scripts/update_benchmark_docs.sh

          # Check if there are changes to commit
          if git diff --quiet docs/src/benchmark.md; then
            echo "No changes to benchmark documentation"
          else
            echo "Committing updated benchmark documentation"
            git config --global user.name "GitHub Actions"
            git config --global user.email "github-actions[bot]@users.noreply.github.com"
            git add docs/src/benchmark.md
            git commit -m "Update benchmark results [skip ci]"
            git push origin HEAD:${{ github.head_ref }}
          fi

      - name: Cleanup
        if: always()
        run: |
          if [ -n "$OLLAMA_PID" ]; then
            echo "Stopping Ollama server (PID: $OLLAMA_PID)"
            kill $OLLAMA_PID || true
          fi
