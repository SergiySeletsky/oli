name: Benchmark
on:
  workflow_dispatch:
    inputs:
      model:
        description: 'Model to benchmark'
        default: 'qwen2.5-coder:7b'
        required: true
  push:
    branches: [main]
    paths:
      - '.github/workflows/benchmark.yml'
  pull_request:
    branches: [main]
    paths:
      - '.github/workflows/benchmark.yml'

permissions:
  contents: write
  id-token: write
  actions: read
  pull-requests: write

jobs:
  benchmark:
    name: Run Benchmark Tests
    runs-on: [self-hosted, gpu]
    timeout-minutes: 20

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          toolchain: stable

      - name: Set model from inputs or default
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "MODEL=${{ github.event.inputs.model }}" >> $GITHUB_ENV
          else
            echo "MODEL=qwen2.5-coder:14b" >> $GITHUB_ENV
          fi

      - name: Check Ollama server
        run: |
          # Check if Ollama server is running and accessible
          echo "Checking connection to Ollama server..."
          if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
            echo "Ollama server is accessible"
          else
            echo "Error: Cannot connect to Ollama server at http://localhost:11434"
            exit 1
          fi

          # Verify the model is available
          echo "Verifying model ${{ env.MODEL }} is available..."
          if curl -s http://localhost:11434/api/tags | grep -q "${{ env.MODEL }}"; then
            echo "Model ${{ env.MODEL }} is available"
          else
            echo "Warning: Model ${{ env.MODEL }} may not be available. Continuing anyway as it might be known by a different name."
          fi

      - name: Cache cargo registry
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-benchmark-${{ hashFiles('**/Cargo.lock') }}

      - name: Build only what's needed
        run: |
          export RUSTFLAGS="-C codegen-units=16 -C opt-level=1"
          cargo build --release --bin oli-server --features "benchmark"

      - name: Setup test environment
        run: |
          # Create results directory
          mkdir -p benchmark_results/tool_tests

          # Create config for Ollama
          mkdir -p ~/.config/oli
          cat > ~/.config/oli/config.json << EOF
          {
            "default_provider": "ollama",
            "default_model": "${{ env.MODEL }}"
          }
          EOF

          # Set dummy API keys
          echo "ANTHROPIC_API_KEY=dummy-key" >> $GITHUB_ENV
          echo "OPENAI_API_KEY=dummy-key" >> $GITHUB_ENV
          echo "GEMINI_API_KEY=dummy-key" >> $GITHUB_ENV

      - name: Run minimal benchmark test
        id: tool_benchmark
        timeout-minutes: 20
        run: |
          # Set environment variables
          export OLLAMA_API_BASE="http://localhost:11434"
          export DEFAULT_PROVIDER="ollama"
          export DEFAULT_MODEL="${{ env.MODEL }}"
          export OLLAMA_SYSTEM_CONTEXT_LENGTH="4096"
          export OLI_TEST_TIMEOUT="90"
          export OLI_BENCHMARK_SUBSET="true"
          export RUST_LOG="info"

          # Special handling for Qwen models
          if [[ "${{ env.MODEL }}" == *"qwen"* ]]; then
            export OLLAMA_FUNCTION_CALLING_FORMAT="verbose"
          fi

          # Run the optimized file read tool test
          echo "Running file read tool benchmark test..."
          START=$(date +%s%3N)
          set +e  # Don't exit on error
          RESULT=$(cargo test --release --features benchmark --test mod -- \
            integration::test_file_read_tool::test_read_file_tool \
            --test-threads=1 -- --nocapture 2>&1)
          TEST_EXIT_CODE=$?
          END=$(date +%s%3N)
          TIME=$((END - START))
          set -e  # Return to exit on error

          # Extract test details
          # Count total and successful tests
          TOTAL_TESTS=$(echo "$RESULT" | grep -c "test integration::test_file_read_tool::" || echo "0")
          SUCCESS_COUNT=$(echo "$RESULT" | grep "test integration::test_file_read_tool::" | grep -c "ok" || echo "0")
          TEST_TIME=$(echo "$RESULT" | grep -o "finished in [0-9.]\+s" | head -1 | grep -o "[0-9.]\+" || echo "")

          # Save results as JSON
          cat > benchmark_results/tool_tests/file_read_tool_results.json << EOF
          {
            "raw_output": $(echo "$RESULT" | jq -Rs .),
            "test_details": {
              "total_tests": $TOTAL_TESTS,
              "successful_tests": $SUCCESS_COUNT,
              "test_time_seconds": "$TEST_TIME",
              "capabilities": {
                "reads_files": $(if echo "$RESULT" | grep -q "test_read_file_tool.*ok"; then echo "true"; else echo "false"; fi),
                "extracts_specific_lines": $(if echo "$RESULT" | grep -i -q "line 2"; then echo "true"; else echo "false"; fi)
              }
            }
          }
          EOF

          # Create summary files
          TEST_STATUS="failed"
          if [ $TEST_EXIT_CODE -eq 0 ]; then
            TEST_STATUS="completed"
          fi

          cat > benchmark_results/tool_tests/summary.json << EOF
          {
            "metadata": {
              "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
              "model": "${{ env.MODEL }}",
              "test_type": "file_read_benchmark"
            },
            "metrics": {
              "execution_time_ms": $TIME,
              "exit_code": $TEST_EXIT_CODE,
              "success_rate": $(awk "BEGIN {if ($TOTAL_TESTS == 0) print 0; else print ($SUCCESS_COUNT/$TOTAL_TESTS)}")
            },
            "result": "$TEST_STATUS"
          }
          EOF

          cat > benchmark_results/summary.json << EOF
          {
            "tool_benchmark_ms": $TIME,
            "model": "${{ env.MODEL }}",
            "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
            "status": "$TEST_STATUS",
            "exit_code": $TEST_EXIT_CODE,
            "test_summary": {
              "total": $TOTAL_TESTS,
              "passed": $SUCCESS_COUNT,
              "success_rate": $(awk "BEGIN {print ($SUCCESS_COUNT/$TOTAL_TESTS)}")
            }
          }
          EOF

          # Output summary
          echo "=== Benchmark Results ==="
          echo "Model: ${{ env.MODEL }}"
          echo "Duration: ${TIME}ms"
          echo "Status: $TEST_STATUS (exit code: $TEST_EXIT_CODE)"
          echo "Tests: $SUCCESS_COUNT/$TOTAL_TESTS passed"

          # Store for other steps
          echo "tool_benchmark_time=${TIME}" >> $GITHUB_OUTPUT
          echo "tool_benchmark_status=${TEST_STATUS}" >> $GITHUB_OUTPUT

      - name: Upload results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results
          path: benchmark_results/

  update-docs:
    name: Update Benchmark Documentation
    needs: benchmark
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results
          path: benchmark_results/

      - name: Generate updated benchmark docs
        run: |
          chmod +x .github/scripts/update_benchmark_docs.sh
          .github/scripts/update_benchmark_docs.sh

      - name: Check for changes
        id: check_changes
        run: |
          if ! git diff --quiet docs/src/benchmark.md; then
            echo "has_changes=true" >> $GITHUB_OUTPUT
          else
            echo "has_changes=false" >> $GITHUB_OUTPUT
          fi

      - name: Create Pull Request
        if: steps.check_changes.outputs.has_changes == 'true'
        uses: peter-evans/create-pull-request@v6
        with:
          token: ${{ secrets.BENCHMARK_PAT }}
          commit-message: "Update benchmark results [skip ci]"
          title: "Update benchmark results from latest run"
          body: |
            This PR updates the benchmark documentation with the latest results.

            - Generated automatically from benchmark run
            - Updated docs/src/benchmark.md with latest metrics
            - [Skip CI] to avoid redundant workflows
          branch: update-benchmark-docs
          base: main
          delete-branch: true

      - name: Cleanup
        if: always()
        run: echo "Documentation update complete"
