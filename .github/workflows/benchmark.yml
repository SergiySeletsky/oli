name: Benchmark
on:
  workflow_dispatch:
    inputs:
      model:
        description: 'Model to benchmark'
        default: 'qwen2.5-coder:7b'
        required: true
      prompt:
        description: 'Test prompt to send'
        default: 'Hello, how are you today?'
        required: true
  push:
    branches: [main]
    paths:
      - '.github/workflows/benchmark.yml'
  pull_request:
    branches: [main]
    paths:
      - '.github/workflows/benchmark.yml'

permissions:
  contents: read
  id-token: write
  actions: read

jobs:
  benchmark:
    name: Benchmark Ollama with oli
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Rust
        uses: actions-rs/toolchain@v1
        with:
          profile: minimal
          toolchain: stable
          override: true

      - name: Set defaults for non-workflow_dispatch events
        if: github.event_name != 'workflow_dispatch'
        run: |
          echo "MODEL=qwen2.5-coder:7b" >> $GITHUB_ENV
          echo "PROMPT=Hello, how are you today?" >> $GITHUB_ENV

      - name: Set values from workflow inputs
        if: github.event_name == 'workflow_dispatch'
        run: |
          echo "MODEL=${{ github.event.inputs.model }}" >> $GITHUB_ENV
          echo "PROMPT=${{ github.event.inputs.prompt }}" >> $GITHUB_ENV

      # Cache the Ollama models to avoid redownloading each time
      - name: Cache Ollama models
        id: cache-ollama
        uses: actions/cache@v3
        with:
          path: ~/.ollama/models
          key: ollama-${{ env.MODEL }}-${{ hashFiles('.github/workflows/benchmark.yml') }}

      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          ollama --version

      - name: Start Ollama server
        id: server
        run: |
          # Create Ollama directories if the cache missed
          mkdir -p ~/.ollama/models

          # Start Ollama in background
          ollama serve &
          echo "OLLAMA_PID=$!" >> $GITHUB_ENV

          # Wait for server to be ready
          echo "Waiting for Ollama server to start..."
          start_time=$(date +%s)
          max_wait=30

          until curl -s http://localhost:11434/api/tags > /dev/null 2>&1; do
            sleep 1
            elapsed=$(($(date +%s) - start_time))
            if [ $elapsed -ge $max_wait ]; then
              echo "::error::Timed out waiting for Ollama server to start"
              exit 1
            fi
          done

          echo "Ollama server started successfully"

      - name: Pull model
        run: |
          echo "Pulling model ${{ env.MODEL }}..."
          if [ "${{ steps.cache-ollama.outputs.cache-hit }}" == "true" ]; then
            echo "Using cached model"
          fi
          ollama pull ${{ env.MODEL }}

      - name: Build oli server
        run: |
          echo "Building oli server..."
          # Run faster build for CI
          cargo build --release --bin oli-server
          echo "oli server built successfully"

      - name: Run direct Ollama test
        id: ollama_test
        run: |
          # Create results directory
          mkdir -p benchmark_results

          # Record timestamp
          timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

          # Initialize results structure
          cat > benchmark_results/ollama_result.json << EOF
          {
            "metadata": {
              "timestamp": "$timestamp",
              "model": "${{ env.MODEL }}",
              "prompt": "${{ env.PROMPT }}",
              "type": "direct_ollama_call"
            },
            "metrics": {
              "response_time_ms": 0
            },
            "response": ""
          }
          EOF

          # Run direct Ollama test
          echo "Running direct test with model ${{ env.MODEL }}"
          START=$(date +%s%3N)
          RESPONSE=$(curl -s -X POST http://localhost:11434/api/generate \
            -H "Content-Type: application/json" \
            -d '{
              "model": "${{ env.MODEL }}",
              "prompt": "${{ env.PROMPT }}",
              "stream": false
            }')
          END=$(date +%s%3N)
          TIME=$((END - START))

          # Save raw response for debugging
          echo "$RESPONSE" > benchmark_results/ollama_raw_response.json

          # Check if response is valid
          if ! echo "$RESPONSE" | jq -e '.response' > /dev/null; then
            echo "::error::Failed to get valid response from model"
            cat benchmark_results/ollama_raw_response.json
            exit 1
          fi

          # Extract response text
          RESPONSE_TEXT=$(echo "$RESPONSE" | jq -r '.response')

          # Update results JSON
          jq --arg time "$TIME" \
             --arg response "$RESPONSE_TEXT" \
             '.metrics.response_time_ms = ($time | tonumber) |
              .response = $response' \
             benchmark_results/ollama_result.json > temp.json && mv temp.json benchmark_results/ollama_result.json

          # Output summary
          echo "Direct Ollama test completed successfully in ${TIME}ms"
          echo "ollama_time=${TIME}" >> $GITHUB_OUTPUT

      - name: Create config files for oli server
        id: oli_config
        run: |
          # Create directory for config files
          mkdir -p ~/.config/oli

          # Create a more detailed config file for Ollama that ensures model selection
          cat > ~/.config/oli/config.json << EOF
          {
            "default_provider": "ollama",
            "default_model": "${{ env.MODEL }}",
            "providers": {
              "ollama": {
                "base_url": "http://localhost:11434",
                "api_key": "",
                "default_model": "${{ env.MODEL }}",
                "models": [
                  {
                    "name": "${{ env.MODEL }}",
                    "id": "${{ env.MODEL }}",
                    "context_length": 8192
                  }
                ]
              }
            }
          }
          EOF

          # Also set environment variables directly
          echo "OLLAMA_API_BASE=http://localhost:11434" >> $GITHUB_ENV

          echo "Created oli config at ~/.config/oli/config.json"
          cat ~/.config/oli/config.json

          # Create .env file in the repository directory
          cat > .env << EOF
          OLLAMA_API_BASE=http://localhost:11434
          DEFAULT_MODEL=${{ env.MODEL }}
          DEFAULT_PROVIDER=ollama
          EOF

          echo "Created .env file for additional configuration"
          cat .env

      - name: Run oli server test
        id: oli_test
        run: |
          # Create results directory and set up
          mkdir -p benchmark_results

          # Record timestamp
          timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

          # Initialize results JSON
          cat > benchmark_results/oli_result.json << EOF
          {
            "metadata": {
              "timestamp": "$timestamp",
              "model": "${{ env.MODEL }}",
              "prompt": "${{ env.PROMPT }}",
              "type": "oli_server_call"
            },
            "metrics": {
              "response_time_ms": 0
            },
            "response": ""
          }
          EOF

          # Create a script that ensures proper newline is sent
          cat > run_oli_test.sh << 'EOF'
          #!/bin/bash
          # Uses the config file and sends a properly formatted JSON request
          PROMPT="$1"
          OLLAMA_MODEL="$2"

          # Export Ollama API URL as environment variable
          export OLLAMA_API_BASE="http://localhost:11434"

          # First, let's get the list of available models to find the correct index
          echo "Getting available models..."
          MODELS_RESPONSE=$(printf '{"jsonrpc":"2.0","id":1,"method":"get_available_models","params":{}}\n' | \
            OLLAMA_API_BASE=http://localhost:11434 ./target/release/oli-server 2>/dev/null | head -n 1)

          # Save models response for debugging
          echo "$MODELS_RESPONSE" > models_response.json

          # Find our specific model in the response to get its index
          # First try to match with "(local)" suffix which is how Ollama models are listed
          MODEL_INDEX=$(echo "$MODELS_RESPONSE" | jq -r --arg MODEL "$OLLAMA_MODEL" '.result.models | map(.name | contains($MODEL) and contains("(local)")) | index(true) // -1')

          # If we didn't find it with (local) suffix, try just the model name
          if [ "$MODEL_INDEX" = "-1" ]; then
            MODEL_INDEX=$(echo "$MODELS_RESPONSE" | jq -r --arg MODEL "$OLLAMA_MODEL" '.result.models | map(.name | contains($MODEL) or .id | contains($MODEL)) | index(true) // 3')
          fi
          echo "Found model index for $OLLAMA_MODEL: $MODEL_INDEX"

          # Create a properly formatted request with a trailing newline
          # Use the discovered model index or fallback to 3 (assuming Ollama models start after the cloud models)
          MODEL="${OLLAMA_MODEL}"
          printf '{"jsonrpc":"2.0","id":1,"method":"query_model","params":{"prompt":"%s","model_index":%s,"use_agent":false}}\n' "$PROMPT" "$MODEL_INDEX" > request.txt

          # Run the server once with the request with explicit environment variables
          # Redirect stderr to a temporary log file so we can capture more detailed logs
          echo "Running oli server with model index $MODEL_INDEX..."
          cat request.txt | OLLAMA_API_BASE=http://localhost:11434 \
                           DEFAULT_MODEL="${MODEL}" \
                           DEFAULT_PROVIDER="ollama" \
                           RUST_LOG=debug \
                           ./target/release/oli-server 2>oli_server.log | head -n 1
          EOF

          # Make it executable
          chmod +x run_oli_test.sh

          # Run the test with timing
          echo "Running test through oli server with model ${{ env.MODEL }}"
          START=$(date +%s%3N)

          # Note: using quotes to ensure prompt is passed correctly
          RESPONSE=$(./run_oli_test.sh "${{ env.PROMPT }}" "${{ env.MODEL }}")

          END=$(date +%s%3N)
          TIME=$((END - START))

          # Save raw response for debugging
          echo "$RESPONSE" > benchmark_results/oli_raw_response.json

          # Check if response is valid
          if ! echo "$RESPONSE" | jq -e '.result.response' > /dev/null; then
            echo "::error::Failed to get valid response from oli server"
            echo "Raw response:"
            cat benchmark_results/oli_raw_response.json
            echo "Models response:"
            cat models_response.json
            echo "Request sent:"
            cat request.txt
            echo "Server log:"
            cat oli_server.log
            # Print environment variables for debugging
            echo "Environment variables:"
            echo "MODEL=${{ env.MODEL }}"
            echo "MODEL_INDEX=$MODEL_INDEX"
            echo "OLLAMA_API_BASE=$OLLAMA_API_BASE"

            # Additional diagnostic - list ollama models directly
            echo "Directly checking Ollama models:"
            curl -s http://localhost:11434/api/tags | jq '.'

            # As a last resort, try to query the directly
            echo "Trying direct Ollama request as a test:"
            curl -s -X POST http://localhost:11434/api/generate \
              -H "Content-Type: application/json" \
              -d "{\"model\":\"$OLLAMA_MODEL\",\"prompt\":\"test\",\"stream\":false}" | jq '.'

            exit 1
          fi

          # Extract response text
          RESPONSE_TEXT=$(echo "$RESPONSE" | jq -r '.result.response')

          # Update results JSON
          jq --arg time "$TIME" \
             --arg response "$RESPONSE_TEXT" \
             '.metrics.response_time_ms = ($time | tonumber) |
              .response = $response' \
             benchmark_results/oli_result.json > temp.json && mv temp.json benchmark_results/oli_result.json

          # Output summary
          echo "oli server test completed successfully in ${TIME}ms"
          echo "oli_time=${TIME}" >> $GITHUB_OUTPUT

      - name: Compare results
        run: |
          # Combine results into a summary
          OLLAMA_TIME=$(jq '.metrics.response_time_ms' benchmark_results/ollama_result.json)
          OLI_TIME=$(jq '.metrics.response_time_ms' benchmark_results/oli_result.json)

          # Calculate overhead
          OVERHEAD=$((OLI_TIME - OLLAMA_TIME))
          PERCENT_OVERHEAD=$(echo "scale=2; $OVERHEAD * 100 / $OLLAMA_TIME" | bc)

          # Create summary file
          cat > benchmark_results/summary.json << EOF
          {
            "direct_ollama_ms": $OLLAMA_TIME,
            "oli_server_ms": $OLI_TIME,
            "overhead_ms": $OVERHEAD,
            "overhead_percent": $PERCENT_OVERHEAD,
            "model": "${{ env.MODEL }}",
            "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
          }
          EOF

          # Output summary to console
          echo "=== Benchmark Results ==="
          echo "Direct Ollama time: ${OLLAMA_TIME}ms"
          echo "oli server time: ${OLI_TIME}ms"
          echo "Overhead: ${OVERHEAD}ms (${PERCENT_OVERHEAD}%)"

          # Print oli server log for diagnostics
          echo "=== oli Server Log ==="
          cat oli_server.log

      - name: Upload results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results
          path: |
            benchmark_results/
            oli_server.log
            ~/.config/oli/config.json

      - name: Cleanup
        if: always()
        run: |
          if [ -n "$OLLAMA_PID" ]; then
            echo "Stopping Ollama server (PID: $OLLAMA_PID)"
            kill $OLLAMA_PID || true
          fi
