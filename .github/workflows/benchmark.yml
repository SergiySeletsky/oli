name: Benchmark
on:
  workflow_dispatch:
    inputs:
      model:
        description: 'Model to benchmark'
        default: 'qwen2.5-coder:7b'
        required: true
      prompt:
        description: 'Test prompt to send'
        default: 'Hello, how are you today?'
        required: true
  push:
    branches: [main]
    paths:
      - '.github/workflows/benchmark.yml'
  pull_request:
    branches: [main]
    paths:
      - '.github/workflows/benchmark.yml'

permissions:
  contents: read
  id-token: write
  actions: read

jobs:
  benchmark:
    name: Benchmark Ollama with oli
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Rust
        uses: actions-rs/toolchain@v1
        with:
          profile: minimal
          toolchain: stable
          override: true

      - name: Set defaults for non-workflow_dispatch events
        if: github.event_name != 'workflow_dispatch'
        run: |
          echo "MODEL=qwen2.5-coder:7b" >> $GITHUB_ENV
          echo "PROMPT=Hello, how are you today?" >> $GITHUB_ENV

      - name: Set values from workflow inputs
        if: github.event_name == 'workflow_dispatch'
        run: |
          echo "MODEL=${{ github.event.inputs.model }}" >> $GITHUB_ENV
          echo "PROMPT=${{ github.event.inputs.prompt }}" >> $GITHUB_ENV

      # Cache the Ollama models to avoid redownloading each time
      - name: Cache Ollama models
        id: cache-ollama
        uses: actions/cache@v3
        with:
          path: ~/.ollama/models
          key: ollama-${{ env.MODEL }}-${{ hashFiles('.github/workflows/benchmark.yml') }}

      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          ollama --version

      - name: Start Ollama server
        id: server
        run: |
          # Create Ollama directories if the cache missed
          mkdir -p ~/.ollama/models

          # Start Ollama in background
          ollama serve &
          echo "OLLAMA_PID=$!" >> $GITHUB_ENV

          # Wait for server to be ready
          echo "Waiting for Ollama server to start..."
          start_time=$(date +%s)
          max_wait=30

          until curl -s http://localhost:11434/api/tags > /dev/null 2>&1; do
            sleep 1
            elapsed=$(($(date +%s) - start_time))
            if [ $elapsed -ge $max_wait ]; then
              echo "::error::Timed out waiting for Ollama server to start"
              exit 1
            fi
          done

          echo "Ollama server started successfully"

      - name: Pull model
        run: |
          echo "Pulling model ${{ env.MODEL }}..."
          if [ "${{ steps.cache-ollama.outputs.cache-hit }}" == "true" ]; then
            echo "Using cached model"
          fi
          ollama pull ${{ env.MODEL }}

      - name: Build oli server
        run: |
          echo "Building oli server..."
          # Run faster build for CI
          cargo build --release --bin oli-server
          echo "oli server built successfully"

      - name: Run direct Ollama test
        id: ollama_test
        run: |
          # Create results directory
          mkdir -p benchmark_results

          # Record timestamp
          timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

          # Initialize results structure
          cat > benchmark_results/ollama_result.json << EOF
          {
            "metadata": {
              "timestamp": "$timestamp",
              "model": "${{ env.MODEL }}",
              "prompt": "${{ env.PROMPT }}",
              "type": "direct_ollama_call"
            },
            "metrics": {
              "response_time_ms": 0
            },
            "response": ""
          }
          EOF

          # Run direct Ollama test
          echo "Running direct test with model ${{ env.MODEL }}"
          START=$(date +%s%3N)
          RESPONSE=$(curl -s -X POST http://localhost:11434/api/generate \
            -H "Content-Type: application/json" \
            -d '{
              "model": "${{ env.MODEL }}",
              "prompt": "${{ env.PROMPT }}",
              "stream": false
            }')
          END=$(date +%s%3N)
          TIME=$((END - START))

          # Save raw response for debugging
          echo "$RESPONSE" > benchmark_results/ollama_raw_response.json

          # Check if response is valid
          if ! echo "$RESPONSE" | jq -e '.response' > /dev/null; then
            echo "::error::Failed to get valid response from model"
            cat benchmark_results/ollama_raw_response.json
            exit 1
          fi

          # Extract response text
          RESPONSE_TEXT=$(echo "$RESPONSE" | jq -r '.response')

          # Update results JSON
          jq --arg time "$TIME" \
             --arg response "$RESPONSE_TEXT" \
             '.metrics.response_time_ms = ($time | tonumber) |
              .response = $response' \
             benchmark_results/ollama_result.json > temp.json && mv temp.json benchmark_results/ollama_result.json

          # Output summary
          echo "Direct Ollama test completed successfully in ${TIME}ms"
          echo "ollama_time=${TIME}" >> $GITHUB_OUTPUT

      - name: Run oli server test
        id: oli_test
        run: |
          # Create results directory and set up
          mkdir -p benchmark_results

          # Record timestamp
          timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

          # Initialize results JSON
          cat > benchmark_results/oli_result.json << EOF
          {
            "metadata": {
              "timestamp": "$timestamp",
              "model": "${{ env.MODEL }}",
              "prompt": "${{ env.PROMPT }}",
              "type": "oli_server_call"
            },
            "metrics": {
              "response_time_ms": 0
            },
            "response": ""
          }
          EOF

          # Create a script that ensures proper newline is sent
          cat > run_oli_test.sh << 'EOF'
          #!/bin/bash
          # Takes a prompt and sends a properly formatted JSON-RPC request to oli server
          PROMPT="$1"

          # Create a properly formatted request with a trailing newline - critical!
          printf '{"jsonrpc":"2.0","id":1,"method":"query_model","params":{"prompt":"%s","model_index":0,"use_agent":false}}\n' "$PROMPT" > request.txt

          # Run the server once with the request (this is the key: one request per server instance)
          cat request.txt | ./target/release/oli-server 2>oli_server.log | head -n 1
          EOF

          # Make it executable
          chmod +x run_oli_test.sh

          # Run the test with timing
          echo "Running test through oli server with model ${{ env.MODEL }}"
          START=$(date +%s%3N)

          # Note: using quotes to ensure prompt is passed correctly
          RESPONSE=$(./run_oli_test.sh "${{ env.PROMPT }}")

          END=$(date +%s%3N)
          TIME=$((END - START))

          # Save raw response for debugging
          echo "$RESPONSE" > benchmark_results/oli_raw_response.json

          # Check if response is valid
          if ! echo "$RESPONSE" | jq -e '.result.response' > /dev/null; then
            echo "::error::Failed to get valid response from oli server"
            cat benchmark_results/oli_raw_response.json
            cat oli_server.log
            exit 1
          fi

          # Extract response text
          RESPONSE_TEXT=$(echo "$RESPONSE" | jq -r '.result.response')

          # Update results JSON
          jq --arg time "$TIME" \
             --arg response "$RESPONSE_TEXT" \
             '.metrics.response_time_ms = ($time | tonumber) |
              .response = $response' \
             benchmark_results/oli_result.json > temp.json && mv temp.json benchmark_results/oli_result.json

          # Output summary
          echo "oli server test completed successfully in ${TIME}ms"
          echo "oli_time=${TIME}" >> $GITHUB_OUTPUT

      - name: Compare results
        run: |
          # Combine results into a summary
          OLLAMA_TIME=$(jq '.metrics.response_time_ms' benchmark_results/ollama_result.json)
          OLI_TIME=$(jq '.metrics.response_time_ms' benchmark_results/oli_result.json)

          # Calculate overhead
          OVERHEAD=$((OLI_TIME - OLLAMA_TIME))
          PERCENT_OVERHEAD=$(echo "scale=2; $OVERHEAD * 100 / $OLLAMA_TIME" | bc)

          # Create summary file
          cat > benchmark_results/summary.json << EOF
          {
            "direct_ollama_ms": $OLLAMA_TIME,
            "oli_server_ms": $OLI_TIME,
            "overhead_ms": $OVERHEAD,
            "overhead_percent": $PERCENT_OVERHEAD,
            "model": "${{ env.MODEL }}",
            "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
          }
          EOF

          # Output summary to console
          echo "=== Benchmark Results ==="
          echo "Direct Ollama time: ${OLLAMA_TIME}ms"
          echo "oli server time: ${OLI_TIME}ms"
          echo "Overhead: ${OVERHEAD}ms (${PERCENT_OVERHEAD}%)"

          # Print oli server log for diagnostics
          echo "=== oli Server Log ==="
          cat oli_server.log

      - name: Upload results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results
          path: |
            benchmark_results/
            oli_server.log

      - name: Cleanup
        if: always()
        run: |
          if [ -n "$OLLAMA_PID" ]; then
            echo "Stopping Ollama server (PID: $OLLAMA_PID)"
            kill $OLLAMA_PID || true
          fi
